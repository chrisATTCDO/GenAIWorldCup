{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8935e1-4db1-4f22-9fa5-6feff28ccaaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install langchain_databricks\n",
    "%pip install databricks-vectorsearch\n",
    "%pip install databricks-sdk\n",
    "%pip install langchain_community\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5578d961-33bf-41c0-8178-55ac89a2369c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import base64\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "from io import BytesIO\n",
    "from typing import List, Any, Optional\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_community.embeddings import DatabricksEmbeddings\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "from mlflow.models import infer_signature, set_signature\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import ColSpec, DataType, ParamSchema, ParamSpec, Schema\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "config_path=\"/config/wallstreet_config.json\"\n",
    "CHAT_HISTORY_COUNT = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6813fe9-4198-45b6-8970-db549227533c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ExtensionLoader:\n",
    "    def __init__(self, ingestion_data: dict):\n",
    "        \"\"\"\n",
    "        Initialize the ExtensionLoader with ingestion data.\n",
    "\n",
    "        :param ingestion_data: Dictionary containing ingestion configuration.\n",
    "        \"\"\"\n",
    "        self.loader_class = ingestion_data.get(\"file_loader_class_name\")\n",
    "        self.loader_kwargs = ingestion_data.get(\"loader_kwargs\")\n",
    "        self.splitter_class = ingestion_data.get(\"splitter_class_name\")\n",
    "        self.splitter_kwargs = ingestion_data.get(\"splitter_kwargs\")\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the Config with a file path.\n",
    "\n",
    "        :param file_path: Path to the configuration file.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise Exception(\"Config File Path Not present\")\n",
    "        self.__domain_data = json.load(open(file_path, \"r\"))\n",
    "        self.__vector_store = self.__domain_data.get(\"vector_store\", None)\n",
    "        self.__generator = self.__domain_data.get(\"generator\", None)\n",
    "        self.__prompt = self.__domain_data.get(\"prompts\", None)\n",
    "        self.__ingestion_configuration = self.__domain_data.get(\"ingestion\", None)\n",
    "        self.__extension_configs = self.__load_extension_list()\n",
    "\n",
    "    def __load_extension_list(self) -> dict:\n",
    "        \"\"\"\n",
    "        Load the extension list from the ingestion configuration.\n",
    "\n",
    "        :return: Dictionary of extension configurations.\n",
    "        \"\"\"\n",
    "        extension_configs = {}\n",
    "        if self.__ingestion_configuration:\n",
    "            for ingestion_data in self.__ingestion_configuration:\n",
    "                for extension in ingestion_data[\"extension\"]:\n",
    "                    extension_configs[extension] = ExtensionLoader(ingestion_data=ingestion_data)\n",
    "        return extension_configs\n",
    "\n",
    "    def get_loader_for_extension(self, extension: str):\n",
    "        \"\"\"\n",
    "        Get the loader for a specific extension.\n",
    "\n",
    "        :param extension: Extension name.\n",
    "        :return: ExtensionLoader object or None.\n",
    "        \"\"\"\n",
    "        return self.__extension_configs.get(extension, None)\n",
    "\n",
    "    def get_embedding_model(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the embedding model name.\n",
    "\n",
    "        :return: Embedding model name.\n",
    "        :raises Exception: If model details are not present.\n",
    "        \"\"\"\n",
    "        if self.__vector_store:\n",
    "            embedding = self.__vector_store.get(\"embedding\", None)\n",
    "            if embedding:\n",
    "                model = embedding.get(\"model\", None)\n",
    "                if model:\n",
    "                    return model\n",
    "        raise Exception(\"Model details Not Present\")\n",
    "\n",
    "    def get_embedding_model_dimension(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the embedding model dimension.\n",
    "\n",
    "        :return: Embedding model dimension.\n",
    "        :raises Exception: If model details are not present.\n",
    "        \"\"\"\n",
    "        if self.__vector_store:\n",
    "            embedding = self.__vector_store.get(\"embedding\", None)\n",
    "            if embedding:\n",
    "                dimension = embedding.get(\"dimension\", None)\n",
    "                if dimension:\n",
    "                    return dimension\n",
    "        raise Exception(\"Model details Not Present\")\n",
    "\n",
    "    def get_vector_index_schema(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get the vector index schema.\n",
    "\n",
    "        :return: Vector index schema.\n",
    "        :raises Exception: If index schema is not present.\n",
    "        \"\"\"\n",
    "        if self.__vector_store:\n",
    "            index = self.__vector_store.get(\"index\", None)\n",
    "            if index:\n",
    "                schema = index.get(\"schema\", None)\n",
    "                if schema:\n",
    "                    return schema\n",
    "        raise Exception(\"Index Schema Not Present\")\n",
    "\n",
    "    def get_vector_index_primary_key(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the vector index primary key.\n",
    "\n",
    "        :return: Vector index primary key.\n",
    "        :raises Exception: If primary key information is not present.\n",
    "        \"\"\"\n",
    "        if self.__vector_store:\n",
    "            index = self.__vector_store.get(\"index\", None)\n",
    "            if index:\n",
    "                primary_key = index.get(\"primary_key\", None)\n",
    "                if primary_key:\n",
    "                    return primary_key\n",
    "        raise Exception(\"Index Primary key information Not Present\")\n",
    "\n",
    "    def get_vector_index_vector_column(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the vector index embedding vector column.\n",
    "\n",
    "        :return: Embedding vector column name.\n",
    "        :raises Exception: If embedding vector column information is not present.\n",
    "        \"\"\"\n",
    "        if self.__vector_store:\n",
    "            index = self.__vector_store.get(\"index\", None)\n",
    "            if index:\n",
    "                embedding_vector_column = index.get(\"embedding_vector_column\", None)\n",
    "                if embedding_vector_column:\n",
    "                    return embedding_vector_column\n",
    "        raise Exception(\"Index Embedding vector column information not Present.\")\n",
    "\n",
    "    def get_vector_endpoint(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the vector endpoint name.\n",
    "\n",
    "        :return: Vector endpoint name.\n",
    "        :raises Exception: If endpoint name details are not present.\n",
    "        \"\"\"\n",
    "        if self.__vector_store:\n",
    "            endpoint_name = self.__vector_store.get(\"endpoint_name\", None)\n",
    "            if endpoint_name:\n",
    "                return endpoint_name\n",
    "        raise Exception(\"Endpoint_name details Not Present\")\n",
    "\n",
    "    def get_vector_index(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the vector index name.\n",
    "\n",
    "        :return: Vector index name.\n",
    "        :raises Exception: If vector index name information is missing.\n",
    "        \"\"\"\n",
    "        if self.__vector_store:\n",
    "            index = self.__vector_store.get(\"index\", None)\n",
    "            if index:\n",
    "                index_name = index.get(\"name\", None)\n",
    "                if index_name:\n",
    "                    return index_name\n",
    "        raise Exception(\"Vector Index Name information missing in the config not Present\")\n",
    "\n",
    "    def get_generator_endpoint(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the generator endpoint name.\n",
    "\n",
    "        :return: Generator endpoint name.\n",
    "        :raises Exception: If LLM endpoint is not present.\n",
    "        \"\"\"\n",
    "        if self.__generator:\n",
    "            endpoint_name = self.__generator.get(\"openai_endpoint\")\n",
    "            if endpoint_name:\n",
    "                return endpoint_name\n",
    "        raise Exception(\"LLM endpoint not present\")\n",
    "\n",
    "    def get_generator_model(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the generator model name.\n",
    "\n",
    "        :return: Generator model name.\n",
    "        :raises Exception: If LLM model is not present.\n",
    "        \"\"\"\n",
    "        if self.__generator:\n",
    "            openai_chat_model = self.__generator.get(\"openai_chat_model\")\n",
    "            if openai_chat_model:\n",
    "                return openai_chat_model\n",
    "        raise Exception(\"LLM Model not present\")\n",
    "\n",
    "    def get_vector_query_type(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the vector query type.\n",
    "\n",
    "        :return: Vector query type.\n",
    "        :raises Exception: If query type is not present.\n",
    "        \"\"\"\n",
    "        if self.__vector_store:\n",
    "            query_type = self.__vector_store.get(\"query_type\")\n",
    "            if query_type:\n",
    "                return query_type\n",
    "        raise Exception(\"Query Type not present\")\n",
    "\n",
    "    def get_generator_prompt(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the generator prompt.\n",
    "\n",
    "        :return: Generator prompt.\n",
    "        :raises Exception: If generator prompt is not present.\n",
    "        \"\"\"\n",
    "        if self.__prompt:\n",
    "            generator_prompt = self.__prompt.get(\"generator_prompt\")\n",
    "            if generator_prompt:\n",
    "                return generator_prompt\n",
    "        raise Exception(\"Generator prompt not present\")\n",
    "\n",
    "    def get_symbol_identifier_prompt(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the symbol identifier prompt.\n",
    "\n",
    "        :return: Symbol identifier prompt.\n",
    "        :raises Exception: If symbol identifier prompt is not present.\n",
    "        \"\"\"\n",
    "        if self.__prompt:\n",
    "            symbol_identifier_prompt = self.__prompt.get(\"symbol_identifier_prompt\")\n",
    "            if symbol_identifier_prompt:\n",
    "                return symbol_identifier_prompt\n",
    "        raise Exception(\"Symbol Identifier prompt not present\")\n",
    "\n",
    "    def get_symbol_conversation_prompt(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the symbol conversation prompt.\n",
    "\n",
    "        :return: Symbol conversation prompt.\n",
    "        :raises Exception: If symbol conversation prompt is not present.\n",
    "        \"\"\"\n",
    "        if self.__prompt:\n",
    "            symbol_conversation_prompt = self.__prompt.get(\"symbol_conversation_prompt\")\n",
    "            if symbol_conversation_prompt:\n",
    "                return symbol_conversation_prompt\n",
    "        raise Exception(\"Symbol Conversation prompt not present\")\n",
    "\n",
    "    def get_multiturn_prompt(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the multi-turn prompt.\n",
    "\n",
    "        :return: Multi-turn prompt.\n",
    "        :raises Exception: If multi-turn prompt is not present.\n",
    "        \"\"\"\n",
    "        if self.__prompt:\n",
    "            multiturn_prompt = self.__prompt.get(\"multiturn_prompt\")\n",
    "            if multiturn_prompt:\n",
    "                return multiturn_prompt\n",
    "        raise Exception(\"Multi-turn prompt not present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9566d06-64e6-4c97-a640-bc7ca694a483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from typing import Optional, List, Dict\n",
    "from PIL import Image\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "class AzureGpt4VService:\n",
    "    \"\"\"\n",
    "    A service class to interact with Azure's GPT-4V model for generating image descriptions.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    __llm_endpoint : str\n",
    "        The endpoint URL for the GPT-4V model.\n",
    "    __llm_api_key : str\n",
    "        The API key for authenticating requests to the GPT-4V model.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    get_image_format(base64_source: str) -> str:\n",
    "        Determines the format of a base64-encoded image.\n",
    "    image_description(images: List[str], prompt: str, detail_mode: str, image_urls: List[str] = None, deployment_name: Optional[str] = None, llm_endpoint: Optional[str] = None, llm_api_version: Optional[str] = None) -> List[str]:\n",
    "        Generates descriptions for a list of images using the GPT-4V model.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_endpoint: Optional[str] = None, llm_api_key: Optional[str] = None):\n",
    "        self.__llm_endpoint = llm_endpoint\n",
    "        self.__llm_api_key = llm_api_key\n",
    "\n",
    "    def get_image_format(self, base64_source: str) -> str:\n",
    "        \"\"\"\n",
    "        Determines the format of a base64-encoded image.\n",
    "\n",
    "        :param base64_source: Base64 encoded image string.\n",
    "        :return: Image format.\n",
    "        \"\"\"\n",
    "        image_stream = BytesIO(base64.b64decode(base64_source))\n",
    "        image = Image.open(image_stream)\n",
    "        return image.format\n",
    "\n",
    "    def image_description(\n",
    "        self,\n",
    "        images: List[str],\n",
    "        prompt: str,\n",
    "        detail_mode: str,\n",
    "        image_urls: List[str] = None,\n",
    "        deployment_name: Optional[str] = None,\n",
    "        llm_endpoint: Optional[str] = None,\n",
    "        llm_api_version: Optional[str] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates descriptions for a list of images using the GPT-4V model.\n",
    "\n",
    "        :param images: List of image URLs.\n",
    "        :param prompt: Prompt for the GPT-4V model.\n",
    "        :param detail_mode: Detail mode for the image description.\n",
    "        :param image_urls: List of base64 encoded image URLs.\n",
    "        :param deployment_name: Optional deployment name.\n",
    "        :param llm_endpoint: Optional LLM endpoint.\n",
    "        :param llm_api_version: Optional LLM API version.\n",
    "        :return: List of image descriptions.\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "        content = []\n",
    "        for i, image in enumerate(image_urls):\n",
    "            image_format = self.get_image_format(image).lower()\n",
    "            if image:\n",
    "                content.append({\"type\": \"text\", \"text\": f\"!()[{image_urls[i][0]}]\"})\n",
    "            content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/{image_format};base64,{image}\", \"detail\": detail_mode}})\n",
    "            messages.append({\"role\": \"user\", \"content\": content})\n",
    "            payload = json.dumps({\"messages\": messages, \"enhancements\": {\"ocr\": {\"enabled\": False}, \"grounding\": {\"enabled\": True}}, \"temperature\": 0.1, \"max_tokens\": 1000})\n",
    "            headers = {\n",
    "                            'api-key': self.__llm_api_key,\n",
    "                            'Content-Type': 'application/json'\n",
    "                        }\n",
    "            response = requests.post(self.__llm_endpoint, headers=headers, data=payload)\n",
    "            response = json.loads(response.text)\n",
    "            page_content = response['choices'][0]['message']['content']\n",
    "        return page_content\n",
    "    \n",
    "\n",
    "class Chat:\n",
    "    def __init__(self):\n",
    "        self.__vs_client = VectorSearchClient(disable_notice=True)\n",
    "        self.__deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "        self.__config = Config(config_path)\n",
    "        self.__index = self.__vs_client.get_index(\n",
    "            index_name=self.__config.get_vector_index(),\n",
    "            endpoint_name=self.__config.get_vector_endpoint()\n",
    "        )\n",
    "        self.__llm_endpoint = self.__config.get_generator_endpoint()\n",
    "        self.__llm_api_key = \"\"  # read from databricks secret\n",
    "        self.__vector_query_type = self.__config.get_vector_query_type()\n",
    "\n",
    "    def __get_query_embedding(self, query_text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get the query embedding vector.\n",
    "\n",
    "        :param query_text: Query text.\n",
    "        :return: Query embedding vector.\n",
    "        \"\"\"\n",
    "        response = self.__deploy_client.predict(endpoint=self.__config.get_embedding_model(), inputs={\"input\": query_text})\n",
    "        return response.data[0][\"embedding\"]\n",
    "\n",
    "    def __similarity_search(self, query_text: str, query_filter: dict) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Perform similarity search.\n",
    "\n",
    "        :param query_text: Query text.\n",
    "        :param query_filter: Query filter.\n",
    "        :return: Search results.\n",
    "        \"\"\"\n",
    "        embedding_model = DatabricksEmbeddings(endpoint=\"databricks-gte-large-en\")\n",
    "        vector_search_as_retriever = DatabricksVectorSearch(\n",
    "            self.__index,\n",
    "            text_column=\"page_content\",\n",
    "            columns=[\"id\", \"page_content\", \"metadata\"],\n",
    "            embedding=embedding_model\n",
    "        ).as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    def format_context(docs):\n",
    "        chunk_contents = [f\"Passage: {d.page_content}\\n\" for d in docs]\n",
    "        return \"\".join(chunk_contents)\n",
    "\n",
    "        return vector_search_as_retriever.invoke(query_text)\n",
    "\n",
    "    def __live_qna(self, query: str, image_collection: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Perform live QnA on images.\n",
    "\n",
    "        :param query: Query text.\n",
    "        :param image_collection: List of image URLs.\n",
    "        :return: List of image descriptions.\n",
    "        \"\"\"\n",
    "        contexts = []\n",
    "        for img_url in image_collection:\n",
    "            if os.path.exists(img_url):\n",
    "                with open(img_url, \"rb\") as pil_image:\n",
    "                    buffer = BytesIO(pil_image.read()).getvalue()\n",
    "                    img_encoding = base64.b64encode(buffer).decode()\n",
    "                    gptv4_service = AzureGpt4VService(llm_endpoint=self.__config.get_vision_endpoint(), llm_api_key=self.__llm_api_key)\n",
    "                    image_descriptions = gptv4_service.image_description(\n",
    "                        images=[img_url],\n",
    "                        prompt=f\"explain about the image and get information relative to the query: {query}\",\n",
    "                        detail_mode=\"auto\",\n",
    "                        image_urls=[img_encoding]\n",
    "                    )\n",
    "                    contexts.extend(image_descriptions)\n",
    "        return contexts\n",
    "\n",
    "    def __get_multi_turn_question(self, query: str, chat_history: List[dict]) -> str:\n",
    "        \"\"\"\n",
    "        Get multi-turn question.\n",
    "\n",
    "        :param query: Query text.\n",
    "        :param chat_history: Chat history.\n",
    "        :return: Multi-turn question.\n",
    "        \"\"\"\n",
    "        conversation_string = \"\"\"\n",
    "        history-Q: {question}\n",
    "        history-A: {answer}\n",
    "        \"\"\"\n",
    "        conversation = [\n",
    "            conversation_string.format(question=chat.get(\"query\"), answer=chat.get(\"answer\")) for chat in chat_history\n",
    "        ]\n",
    "        conversation = \"\\n\".join(conversation)\n",
    "        prompt = self.__config.get_muliturn_prompt().format(\n",
    "            conversation=conversation, question=query)\n",
    "        response = self.__get_openai_response(prompt)\n",
    "        return response.strip()\n",
    "\n",
    "    def get_context(self, query: str, query_filter: dict) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Get context for the query.\n",
    "\n",
    "        :param query: Query text.\n",
    "        :param query_filter: Query filter.\n",
    "        :return: Search results.\n",
    "        \"\"\"\n",
    "        return self.__similarity_search(query, query_filter)\n",
    "\n",
    "    def __get_openai_response(self, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Get response from OpenAI.\n",
    "\n",
    "        :param user_prompt: User prompt.\n",
    "        :return: Response text.\n",
    "        \"\"\"\n",
    "        payload = json.dumps({\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.95,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "            \"max_tokens\": 800\n",
    "        })\n",
    "        headers = {\n",
    "            'api-key': self.__llm_api_key,\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        response = requests.post(self.__llm_endpoint, headers=headers, data=payload)\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    def __get_db_instruct_response(self, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Get response from Databricks instruction model.\n",
    "\n",
    "        :param user_prompt: User prompt.\n",
    "        :return: Response text.\n",
    "        \"\"\"\n",
    "        model = ChatDatabricks(\n",
    "            endpoint=\"databricks-meta-llama-3-1-405b-instruct\",\n",
    "            extra_params={\"temperature\": 0.01},\n",
    "        )\n",
    "        return model.invoke(user_prompt).content\n",
    "\n",
    "    def __format_chat_history_for_prompt(self, history: List[dict]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Format chat history for prompt.\n",
    "\n",
    "        :param history: Chat history.\n",
    "        :return: Formatted chat history.\n",
    "        \"\"\"\n",
    "        formatted_chat_history = []\n",
    "        if history:\n",
    "            for chat_message in history:\n",
    "                if chat_message[\"role\"] == \"user\":\n",
    "                    formatted_chat_history.append(HumanMessage(content=chat_message[\"content\"]))\n",
    "                elif chat_message[\"role\"] == \"assistant\":\n",
    "                    formatted_chat_history.append(AIMessage(content=chat_message[\"content\"]))\n",
    "        return formatted_chat_history\n",
    "\n",
    "    def __get_llm_response(self, prompt, context: str, query: str, chat_history: List[dict]) -> str:\n",
    "        \"\"\"\n",
    "        Get response from LLM.\n",
    "\n",
    "        :param prompt: Prompt template.\n",
    "        :param context: Context text.\n",
    "        :param query: Query text.\n",
    "        :param chat_history: Chat history.\n",
    "        :return: Response text.\n",
    "        \"\"\"\n",
    "        input_data_messages = []\n",
    "        if chat_history:\n",
    "            for history in chat_history:\n",
    "                chat_query, chat_answer = history.get(\"query\"), history.get(\"answer\")\n",
    "                input_data_messages.append({'content': chat_query, 'role': 'user'})\n",
    "                input_data_messages.append({'content': chat_answer, 'role': 'assistant'})\n",
    "        chat_history = self.__format_chat_history_for_prompt(input_data_messages)\n",
    "        updated_prompt = prompt.invoke(\n",
    "            {\n",
    "                \"chat_history\": chat_history,\n",
    "                \"question\": query,\n",
    "                \"context\": context\n",
    "            }\n",
    "        )\n",
    "\n",
    "        openai = True  # Openai Model\n",
    "        if openai:\n",
    "            user_prompt = prompt.format(context=context, question=query, chat_history=chat_history)\n",
    "            return self.__get_openai_response(user_prompt)\n",
    "        else:\n",
    "            return self.__get_db_instruct_response(updated_prompt)\n",
    "\n",
    "    def predict(self, query: str, chat_history: List[dict] = [], query_filter: dict = {}) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Predict the response for a query.\n",
    "\n",
    "        :param query: Query text.\n",
    "        :param chat_history: Chat history.\n",
    "        :param query_filter: Query filter.\n",
    "        :return: Response and citations.\n",
    "        \"\"\"\n",
    "        if chat_history:\n",
    "            query = self.__get_multi_turn_question(query, chat_history)\n",
    "        results = self.get_context(query, query_filter)\n",
    "        image_list, contexts, citations = [], [], []\n",
    "        for context in results:\n",
    "            contexts.append(context.page_content)\n",
    "            file_path = context.metadata.get(\"source\")\n",
    "            citations.append({\"content\": context.page_content, \"source\": file_path})\n",
    "            if file_path and file_path.lower().endswith(\".jpeg\"):\n",
    "                image_list.append(file_path)\n",
    "        if image_list:\n",
    "            image_context = self.__live_qna(query, image_list)\n",
    "            contexts += image_context\n",
    "        final_context = \"\\n\".join(contexts)\n",
    "        prompt = PromptTemplate(\n",
    "            template=self.__config.get_generator_prompt(),\n",
    "            input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "        )\n",
    "        result = self.__get_llm_response(prompt, final_context, query, chat_history)\n",
    "        return {\"response\": result, \"citations\": citations, \"query\": query}\n",
    "\n",
    "\n",
    "class RagChat(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self):\n",
    "        self.chat = Chat()\n",
    "\n",
    "    def predict(self, context, input_data: pd.DataFrame) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Predict the response for a query using RagChat.\n",
    "\n",
    "        :param context: Context (not used).\n",
    "        :param input_data: Input data containing query and chat history.\n",
    "        :return: Response and citations.\n",
    "        \"\"\"\n",
    "        query = input_data[\"query\"].values[0]\n",
    "        chat_history = input_data[\"chat_history\"].values[0]\n",
    "        input_data_messages = []\n",
    "        if chat_history:\n",
    "            chat_history = json.loads(chat_history)\n",
    "            chat_history = chat_history if len(chat_history) <= 2 else chat_history[CHAT_HISTORY_COUNT:]\n",
    "            for history in chat_history:\n",
    "                chat_query, chat_answer = history['query'], history['answer']\n",
    "                input_data_messages.append({'content': chat_query, 'role': 'user'})\n",
    "                input_data_messages.append({'content': chat_answer, 'role': 'assistant'})\n",
    "        input_data_messages.append({'content': query, 'role': 'user'})\n",
    "        input_data = {'messages': input_data_messages}\n",
    "\n",
    "        return self.chat.predict(query, chat_history, query_filter={})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aa0c118-fc98-4ddc-9cb9-088fa0668aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### TEST LOCAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfebb5f9-39a9-4180-b442-badc8d9f9ee0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ragChatWrapper = RagChat()\n",
    "mlflow.models.set_model(model=ragChatWrapper)\n",
    "\n",
    "chat_history = []\n",
    "print(dt.now())\n",
    "query = \"Who is the CEO of ATT?\"\n",
    "query = \"who is major stock holder of ATT?\"\n",
    "input_df = pd.DataFrame({\n",
    "    \"query\": [query],\n",
    "    \"chat_history\": [json.dumps(chat_history)]\n",
    "})\n",
    "predictions = ragChatWrapper.predict(None, input_df)\n",
    "print(query)\n",
    "print(predictions.get(\"response\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd724857-5169-44f5-92c4-b5a2c6c93dfd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### REGISTER MLFLOW MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5c8c59-fd0b-4957-b6ef-2edd4331073e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"query\"),\n",
    "        ColSpec(DataType.string, \"chat_history\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(DataType.string, \"response\"),ColSpec(DataType.string, \"citations\"),ColSpec(DataType.string, \"query\")])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "conda_env = {\n",
    "    \"channels\": [\"defaults\"],\n",
    "    \"dependencies\": [\n",
    "        f\"python=3.11\",\n",
    "        \"pip\",\n",
    "        {\n",
    "            \"pip\": [\n",
    "                f\"databricks-sdk==0.33.0\",\n",
    "                f\"databricks-vectorsearch==0.41\",\n",
    "                f\"langchain_databricks==0.1.0\",\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \"name\": \"wallstreet_env\",\n",
    "}\n",
    "\n",
    "run_id = None\n",
    "mlflow.set_experiment(\"wallstreet_model\")\n",
    "with mlflow.start_run() as run:\n",
    "  run_id= run.info.run_id\n",
    "  mlflow.pyfunc.log_model(artifact_path=\"wallstreet_model\", python_model=ragChatWrapper,#)#,\n",
    "                          signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1b515d8-12d0-41d8-9392-785cdef7ccaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### TEST MLFLOW MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ee774a-e286-41cb-aaa3-176752a0b236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import json\n",
    "model_uri = f'runs:/{run_id}/wallstreet_model'\n",
    "print(model_uri)\n",
    "chat_history = []\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "stock_chat_history = [{\"question\":\"test answer\",\"answer\":\"test Answer\"}]\n",
    "\n",
    "stock_chat_history = [{\"query\":\"highest revenue of AT&T\",\"answer\":\"The highest revenue estimate among these is $130,110,000,000 (130.110 billion USD).\"}]\n",
    "query = \"what is the lowest revenue?\"\n",
    "\n",
    "query_filter = {}\n",
    "input_df = pd.DataFrame({\n",
    "    \"query\": [query],\n",
    "    \"chat_history\": [json.dumps(stock_chat_history)] \n",
    "})\n",
    "predictions = loaded_model.predict(input_df)\n",
    "print(predictions.get(\"response\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "RAG_Chat",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
