{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d45224-9d13-4631-827e-da089d3d9ece",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, CSVLoader, PyPDFLoader, UnstructuredWordDocumentLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain_core.documents import Document\n",
    "import mlflow\n",
    "from typing import List, Any, Optional\n",
    "import json\n",
    "import mlflow.deployments\n",
    "import uuid\n",
    "import time\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import logging as logger\n",
    "import fitz\n",
    "import io\n",
    "import base64\n",
    "import urllib.parse\n",
    "import platform\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import openai\n",
    "from io import BytesIO\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5765b6e2-f579-4a57-8c3f-ec61936660f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install langchain_community pymupdf openai databricks-vectorsearch jq unstructured\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f82cea8-fb53-418d-963f-d6c833919f75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "class ExtensionLoader:\n",
    "    def __init__(self,ingestion_data):\n",
    "        self.loader_class =  ingestion_data.get(\"file_loader_class_name\")\n",
    "        self.loader_kwargs = ingestion_data.get(\"loader_kwargs\")\n",
    "        self.splitter_class = ingestion_data.get(\"splitter_class_name\")\n",
    "        self.splitter_kwargs = ingestion_data.get(\"splitter_kwargs\")\n",
    "\n",
    "class Config:\n",
    "    def __init__(self,file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            raise Exception(\"Cofig File Path Not present\")\n",
    "        self.__domain_data = json.load(open(file_path,\"r\"))\n",
    "        self.__vector_store = self.__domain_data.get(\"vector_store\",None)\n",
    "        self.__generator =  self.__domain_data.get(\"generator\",None)\n",
    "        self.__vision = self.__domain_data.get(\"vision\",None)\n",
    "        self.__prompt = self.__domain_data.get(\"prompts\",None)\n",
    "        self.__ingestion_configuration = self.__domain_data.get(\"ingestion\",None)\n",
    "        self.__extension_configs = self.__load_extesion_list()\n",
    "\n",
    "    def __load_extesion_list(self):        \n",
    "        extension_configs = {}\n",
    "        if self.__ingestion_configuration:\n",
    "            for ingestion_data in self.__ingestion_configuration:\n",
    "                for extension in ingestion_data[\"extension\"]:\n",
    "                    extension_configs[extension] = ExtensionLoader(ingestion_data=ingestion_data)\n",
    "        return extension_configs\n",
    "    \n",
    "    def get_loader_for_extension(self,extension):\n",
    "        return self.__extension_configs.get(extension,None)\n",
    "    \n",
    "    def get_embedding_model(self):\n",
    "        if self.__vector_store:\n",
    "            embedding = self.__vector_store.get(\"embedding\",None)\n",
    "            if embedding:\n",
    "                model = embedding.get(\"model\",None)\n",
    "                if model:\n",
    "                    return model\n",
    "        raise Exception(\"Model details Not Present\")\n",
    "    \n",
    "    def get_embedding_model_dimension(self):\n",
    "        if self.__vector_store:\n",
    "            embedding = self.__vector_store.get(\"embedding\",None)\n",
    "            if embedding:\n",
    "                dimension = embedding.get(\"dimension\",None)\n",
    "                if dimension:\n",
    "                    return dimension\n",
    "        raise Exception(\"Model details Not Present\")\n",
    "    \n",
    "\n",
    "    def get_vector_index_schema(self):\n",
    "        if self.__vector_store:\n",
    "            index = self.__vector_store.get(\"index\",None)\n",
    "            if index:\n",
    "                schema = index.get(\"schema\",None)\n",
    "                if schema:\n",
    "                    return schema\n",
    "        raise Exception(\"Index Schema Not Present\")\n",
    "    \n",
    "    def get_vector_index_primary_key(self):\n",
    "        if self.__vector_store:\n",
    "            index = self.__vector_store.get(\"index\",None)\n",
    "            if index:\n",
    "                primary_key = index.get(\"primary_key\",None)\n",
    "                if primary_key:\n",
    "                    return primary_key\n",
    "        raise Exception(\"Index Primary key information Not Present\")\n",
    "    \n",
    "    def get_vector_index_vector_column(self):\n",
    "        if self.__vector_store:\n",
    "            index = self.__vector_store.get(\"index\",None)\n",
    "            if index:\n",
    "                embedding_vector_column = index.get(\"embedding_vector_column\",None)\n",
    "                if embedding_vector_column:\n",
    "                    return embedding_vector_column\n",
    "        raise Exception(\"Index Embedding vector column information not Present.\")\n",
    "\n",
    "    def get_vector_endpoint(self):\n",
    "        if self.__vector_store:\n",
    "            endpoint_name = self.__vector_store.get(\"endpoint_name\",None)\n",
    "            if endpoint_name:\n",
    "                return endpoint_name\n",
    "        raise Exception(\"Endpoint_name details Not Present\")\n",
    "    \n",
    "    def get_vector_index(self):\n",
    "        if self.__vector_store:\n",
    "            index = self.__vector_store.get(\"index\",None)\n",
    "            if index:\n",
    "                index_name = index.get(\"name\",None)\n",
    "                if index_name:\n",
    "                    return index_name\n",
    "        raise Exception(\"Vector Index Name information missing in the config not Present\")\n",
    "\n",
    "    def get_generator_endpoint(self):\n",
    "        print(self.__generator)\n",
    "        if self.__generator:\n",
    "            endpoint_name = self.__generator.get(\"openai_endpoint\")\n",
    "            if endpoint_name:\n",
    "                return endpoint_name\n",
    "        raise Exception(\"LLM endpoint not present\")\n",
    "\n",
    "    def get_vision_endpoint(self):\n",
    "        print(self.__vision)\n",
    "        if self.__vision:\n",
    "            endpoint_name = self.__vision.get(\"openai_endpoint\")\n",
    "            if endpoint_name:\n",
    "                return endpoint_name\n",
    "        raise Exception(\"Vision endpoint not present\")\n",
    "    \n",
    "    def get_generator_model(self):\n",
    "        if self.__generator:\n",
    "            openai_chat_model = self.__generator.get(\"openai_chat_model\")\n",
    "            if openai_chat_model:\n",
    "                return openai_chat_model\n",
    "        raise Exception(\"LLM Model not present\")\n",
    "    \n",
    "    def get_vector_query_type(self):\n",
    "        if self.__vector_store:\n",
    "            query_type = self.__vector_store.get(\"query_type\")\n",
    "            if query_type:\n",
    "                return query_type\n",
    "        raise Exception(\"Query Type not present\")\n",
    "    \n",
    "    def get_generator_prompt(self):\n",
    "        if self.__prompt:\n",
    "            genrator_prompt = self.__prompt.get(\"generator_prompt\")\n",
    "            if genrator_prompt:\n",
    "                return genrator_prompt\n",
    "        raise Exception(\"Genrator prompt not present\")\n",
    "    \n",
    "    def get_symbol_identifier_prompt(self):\n",
    "        if self.__prompt:\n",
    "            symbol_identifier_prompt = self.__prompt.get(\"symbol_identifier_prompt\")\n",
    "            if symbol_identifier_prompt:\n",
    "                return symbol_identifier_prompt\n",
    "        raise Exception(\"Symbol Identifier prompt not present\")\n",
    "    \n",
    "    def get_symbol_conversation_prompt(self):\n",
    "        if self.__prompt:\n",
    "            symbol_conversation_prompt = self.__prompt.get(\"symbol_conversation_prompt\")\n",
    "            if symbol_conversation_prompt:\n",
    "                return symbol_conversation_prompt\n",
    "        raise Exception(\"Symbol Conversation prompt not present\")\n",
    "    \n",
    "    def get_muliturn_prompt(self):\n",
    "        if self.__prompt:\n",
    "            muliturn_prompt = self.__prompt.get(\"muliturn_prompt\")\n",
    "            if muliturn_prompt:\n",
    "                return muliturn_prompt\n",
    "        raise Exception(\"Muilti-turn prompt not present\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb03c5a-a1d9-4b4b-8161-30e705ec9143",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "  def __init__(self, endpoint: str):\n",
    "    \"\"\"\n",
    "    Genertates embeddings from the model deployed on Azure Databricks model.\n",
    "    \"\"\"\n",
    "    self.deploy_client =  mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    self.endpoint = endpoint\n",
    "\n",
    "  def generate_embeddings(self, text: str) -> List[float]:\n",
    "    try:\n",
    "      response = self.deploy_client.predict(endpoint=self.endpoint, inputs={\"input\": text})\n",
    "      embeddings = response.data[0][\"embedding\"]\n",
    "      return embeddings\n",
    "    except Exception as e:\n",
    "      raise Exception(f\"Error generating embeddings: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "266de73b-4c4c-404b-8613-032e6cc4eb54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class VectorSearchManager:\n",
    "  def __init__(self, config: Config):\n",
    "    \"\"\"\n",
    "    Initializes the vector database for the given index name and endpoint name\n",
    "    \"\"\"\n",
    "    self.__config = config\n",
    "    self.client = VectorSearchClient(disable_notice=True)\n",
    "    self.index_name = self.__config.get_vector_index()\n",
    "    self.endpoint_name = self.__config.get_vector_endpoint()\n",
    "    self.embedding_dimension = self.__config.get_embedding_model_dimension()\n",
    "    self.primary_key = self.__config.get_vector_index_primary_key()\n",
    "    self.embedding_vector_column = self.__config.get_vector_index_vector_column()\n",
    "    self.index = None\n",
    "    if not self.index_exists():\n",
    "        self.create_index()\n",
    "    else:\n",
    "      self.index = self.client.get_index(\n",
    "        index_name = self.index_name,\n",
    "        endpoint_name = self.endpoint_name\n",
    "    )\n",
    "\n",
    "  def index_exists(self) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the index exists\n",
    "    \"\"\"\n",
    "    try:\n",
    "      existing_index = self.client.list_indexes(self.endpoint_name)      \n",
    "      return self.index_name in [index[\"name\"] for index in existing_index[\"vector_indexes\"]]\n",
    "    except Exception as e:\n",
    "      return False\n",
    "    \n",
    "  def get_index(self):\n",
    "    return self.index\n",
    "\n",
    "  def create_index(self):\n",
    "    \"\"\"\n",
    "    Creates the vector database index\n",
    "    \"\"\"\n",
    "\n",
    "    ##TODO: Move the schema to configurations\n",
    "    schema = self.__config.get_vector_index_schema()\n",
    "    for col in filter_columns:\n",
    "      schema[col] = \"string\"\n",
    "    if self.index_exists():\n",
    "      print(f\"Index {self.index_name} already exists\")\n",
    "      return schema\n",
    "    try:\n",
    "      self.index = self.client.create_direct_access_index(endpoint_name=self.endpoint_name, primary_key=self.primary_key, index_name=self.index_name, embedding_dimension=self.embedding_dimension, embedding_vector_column=self.embedding_vector_column, schema=schema)\n",
    "      print(f\"Created index {self.index_name}\")\n",
    "    except Exception as e:\n",
    "      if \"Vector index\" in str(e) and \"is not ready\" in str(e):\n",
    "        print(f\"Index {self.index_name} is not ready. Retrying...\")\n",
    "        time.sleep(10)  # Wait for 10 seconds before retrying\n",
    "        self.create_index()\n",
    "      else:\n",
    "        raise Exception(f\"Error creating index {self.index_name}: {str(e)}\")\n",
    "  \n",
    "  def add_documents(self, documents: List[dict]):\n",
    "    \"\"\"\n",
    "    Adds the documents to the vector database\n",
    "    \"\"\"\n",
    "    try:\n",
    "      index = self.client.get_index(self.endpoint_name, self.index_name)\n",
    "      upload_doc = index.upsert(documents)\n",
    "      if upload_doc[\"status\"] == \"SUCCESS\":\n",
    "        print(f\"{dict(upload_doc)} documents to index {self.index_name}\")     \n",
    "        return True\n",
    "      else:\n",
    "        raise Exception(f\"Error adding documents to index {self.index_name}: {str(upload_doc)}\")\n",
    "    except Exception as e:\n",
    "      raise Exception(f\"Error adding documents to index {self.index_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b03342c-4a37-470a-93fa-cd9301d517eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ImageExtract:\n",
    "  \"\"\"\n",
    "  A class to extract images from a PDF file, save them to a specified directory, and return metadata about the images.\n",
    "\n",
    "  Attributes:\n",
    "  ----------\n",
    "  file_path : str\n",
    "      The path to the PDF file from which images will be extracted.\n",
    "  image_path_prefix : str\n",
    "      A prefix for naming the extracted image files based on the PDF file name.\n",
    "\n",
    "  Methods:\n",
    "  -------\n",
    "  _sanitize_filename(filename: str) -> str:\n",
    "      Sanitizes a filename to be OS-compatible and safe for saving.\n",
    "  _convert_url_to_filename(image_url) -> str:\n",
    "      Converts a URL to a sanitized filename.\n",
    "  get_image_path(image_url, dir_destination_file) -> str:\n",
    "      Constructs the full path for saving an image.\n",
    "  save_image(url: str, image_base64: str, dir_destination_file: str) -> str:\n",
    "      Saves a base64-encoded image to the specified directory.\n",
    "  load_file() -> List[Document]:\n",
    "      Extracts images from the PDF, saves them, and returns metadata about the images.\n",
    "  \"\"\"\n",
    "  def __init__(self, file_path: str):\n",
    "    self.file_path = file_path\n",
    "    self.image_path_prefix =  os.path.basename(file_path).replace(\".\",\"-\").lower()\n",
    "    \n",
    "  def _sanitize_filename(self, filename: str) -> str:\n",
    "        os_name = platform.system()\n",
    "        filename = urllib.parse.unquote(filename)\n",
    "        if os_name != \"Windows\":\n",
    "            filename = unicodedata.normalize('NFKC', filename)\n",
    "        else:\n",
    "            # This is mainly for window machines used by developers\n",
    "            filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('ascii')\n",
    "        filename = re.sub(r'[^\\w\\s.-]', '', filename.lower())\n",
    "        return filename \n",
    "\n",
    "  def _convert_url_to_filename(self, image_url) -> str:\n",
    "        image_url = image_url.replace(\"#unknown-\", \"/\")\n",
    "        path = urllib.parse.urlparse(image_url).path\n",
    "        filename = os.path.basename(path)\n",
    "        sanitized_filename = self._sanitize_filename(filename)\n",
    "        return sanitized_filename \n",
    "\n",
    "  def get_image_path(self, image_url, dir_destination_file) -> str:\n",
    "          image_name = self._convert_url_to_filename(image_url)\n",
    "\n",
    "          image_file_path = os.path.join(\n",
    "              dir_destination_file, \"image\", f\"{image_name}\")\n",
    "          \n",
    "          return image_file_path    \n",
    "\n",
    "  def save_image(self, url: str, image_base64: str, dir_destination_file: str) -> str:\n",
    "        image_dir = os.path.join(dir_destination_file, \"image\")\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        image_path = self.get_image_path(url, dir_destination_file)\n",
    "        image_bytes = base64.b64decode(image_base64)\n",
    "\n",
    "        with open(image_path, 'wb') as file:\n",
    "            file.write(image_bytes)\n",
    "    \n",
    "        return image_path    \n",
    "      \n",
    "  def load_file(self):\n",
    "        doc = fitz.open(self.file_path)\n",
    "        dir_destination_file = os.path.dirname(\n",
    "            self.file_path).replace(\"pending\", \"images\")\n",
    "        image_collection = {}\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            images = page.get_images(full=True)\n",
    "            for img_index, img_info in enumerate(images):\n",
    "                img_index += 1\n",
    "                xref = img_info[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                if base_image:\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "                    image = Image.open(BytesIO(image_bytes))\n",
    "                    if image.getbbox() and image.size[0] > 100 and image.size[1] > 100:  # Check if the image is not blank and larger than 50x50\n",
    "                        buffer = BytesIO(base_image[\"image\"]).getvalue()\n",
    "                        encoded_image = base64.b64encode(buffer).decode()\n",
    "                        image_file_path = os.path.join(\n",
    "                            dir_destination_file, \"image\", f\"{os.path.basename(self.file_path).lower()}_image_page{page_num + 1}_img_nmbr{img_index}.{base_image['ext']}\")\n",
    "                        image_collection[image_file_path] = encoded_image\n",
    "                        self.save_image(image_file_path, encoded_image, dir_destination_file=dir_destination_file)\n",
    "        return [Document(\n",
    "                    page_content=\"\",\n",
    "                    metadata={\n",
    "                        \"source\": self.file_path,\n",
    "                        \"image_collection\": image_collection\n",
    "                    })]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0fcb88e-8a8b-44c6-8822-d79b3c9d1e21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TableExtract:\n",
    "  \"\"\"\n",
    "  A class to extract tables from a PDF file and return them as a list of Document objects.\n",
    "\n",
    "  Attributes:\n",
    "  ----------\n",
    "  file_path : str\n",
    "      The path to the PDF file from which tables will be extracted.\n",
    "\n",
    "  Methods:\n",
    "  -------\n",
    "  load_file() -> List[Document]:\n",
    "      Extracts tables from the PDF, converts them to CSV format, and returns them as a list of Document objects with metadata.\n",
    "  \"\"\"\n",
    "  def __init__(self, file_path: str):\n",
    "    self.file_path = file_path\n",
    "\n",
    "  def load_file(self) -> List[Document]:\n",
    "          doc = fitz.open(self.file_path)\n",
    "          current_page_num = None  # For testing\n",
    "          df_final = {}\n",
    "          df_list = []\n",
    "          for page_num in range(doc.page_count):\n",
    "              page = doc[page_num]\n",
    "              tables = page.find_tables(\n",
    "                  horizontal_strategy=\"text\", vertical_strategy=\"text\")\n",
    "              page_tables_data = []\n",
    "              metadata = {\n",
    "                  \"source\": self.file_path,\n",
    "                  \"file_path\": self.file_path,\n",
    "                  \"total_pages\": len(doc),\n",
    "              }\n",
    "              for table_num, table in enumerate(tables):\n",
    "                  original_result = table.extract()\n",
    "                  df = pd.DataFrame(table.extract())\n",
    "                  df_csv = df.to_csv(sep='|', index=False, header=False)\n",
    "                  page_tables_data.append(df_csv)\n",
    "              df_final[f\"Page_{page_num + 1}\"] = page_tables_data\n",
    "              df_list.append(Document(page_content=str(\n",
    "                  page_tables_data), metadata=metadata))\n",
    "          return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b50aeeb-ec1c-4be0-a625-c8a36e1c9c3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class AzureGpt4VService:\n",
    "    \"\"\"\n",
    "    A service class to interact with Azure's GPT-4V model for generating image descriptions.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    __llm_endpoint : str\n",
    "        The endpoint URL for the GPT-4V model.\n",
    "    __llm_api_key : str\n",
    "        The API key for authenticating requests to the GPT-4V model.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    get_image_format(base64_source: str) -> str:\n",
    "        Determines the format of a base64-encoded image.\n",
    "    image_description(images: list, prompt: str, detail_mode: str, image_urls: list = None, deployment_name: Optional[str] = None, llm_endpoint: Optional[str] = None, llm_apAzureGpt4VServicei_version: Optional[str] = None) -> list:\n",
    "        Generates descriptions for a list of images using the GPT-4V model.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_endpoint: Optional[str] = None,llm_api_key: Optional[str] = None): \n",
    "        self.__llm_endpoint = llm_endpoint\n",
    "        self.__llm_api_key = \"#####\"\n",
    "\n",
    "    def get_image_format(self, base64_source: str):\n",
    "        image_stream = BytesIO(base64.b64decode(base64_source))\n",
    "        image = Image.open(image_stream)\n",
    "        image_format = image.format\n",
    "        return image_format\n",
    "\n",
    "    def image_description(\n",
    "        self,\n",
    "        images: list,\n",
    "        prompt: str,\n",
    "        detail_mode: str,\n",
    "        image_urls: list = None,\n",
    "        deployment_name: Optional[str] = None,\n",
    "        llm_endpoint: Optional[str] = None,\n",
    "        llm_api_version: Optional[str] = None\n",
    "    ):\n",
    "        self.__init__(llm_endpoint, llm_api_version)\n",
    "        messages = []\n",
    "        messages.append({\"role\": \"system\", \"content\": prompt})\n",
    "        documents = []\n",
    "        content = []\n",
    "        for i, image in enumerate(image_urls):\n",
    "            format = self.get_image_format(image).lower()\n",
    "            if image:\n",
    "                content.append({\"type\": \"text\", \"text\": f\"!()[{image_urls[i][0]}]\"})\n",
    "            content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/{format};base64,{image}\", \"detail\": detail_mode}})\n",
    "            messages.append({\"role\": \"user\", \"content\": content})\n",
    "            payload = json.dumps({\"messages\": messages, \"enhancements\": {\"ocr\": {\"enabled\": False}, \"grounding\": {\"enabled\": True}}, \"temperature\": 0.1, \"max_tokens\": 1000})\n",
    "            headers = {\n",
    "                'api-key': self.__llm_api_key,\n",
    "                'Content-Type': 'application/json'\n",
    "            }\n",
    "            response = requests.request(\"POST\", llm_endpoint, headers=headers, data=payload)\n",
    "            response = json.loads(response.text)\n",
    "            if response[\"choices\"][0][\"message\"][\"role\"] == \"assistant\":\n",
    "                documents.append(Document(page_content=response['choices'][0]['message']['content'], metadata={\"source\": images[0]}))\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434f830c-e205-47fc-99c4-6f0857c821dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "class Upload:\n",
    "  \"\"\"\n",
    "  A class to handle the uploading of documents to a vector database.\n",
    "\n",
    "  Attributes:\n",
    "  ----------\n",
    "  file_path : List[str]\n",
    "      List of file paths to be processed.\n",
    "  file_name : str\n",
    "      Name of the file to be processed.\n",
    "  config_path : str\n",
    "      Path to the configuration file.\n",
    "  vector_search_manager : VectorSearchManager\n",
    "      Instance of the VectorSearchManager to manage vector search operations.\n",
    "\n",
    "  Methods:\n",
    "  -------\n",
    "  document_exists(file_path: str) -> bool:\n",
    "      Checks if a document already exists in the vector database.\n",
    "  process_documents() -> list:\n",
    "      Processes and uploads documents to the vector database.\n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        file_name: str,\n",
    "        config_path: str,\n",
    "        filter_param: dict = {}                                                                                                \n",
    "    ):\n",
    "    self.__config = Config(config_path)\n",
    "    self.__file_path = file_path\n",
    "    self.__file_name = file_name   \n",
    "    self.__embedding_generator = EmbeddingGenerator(endpoint=self.__config.get_embedding_model()) #Instance of the EmbeddingGenerator to generate embeddings for documents.    \n",
    "    \n",
    "    self.vector_search_manager = VectorSearchManager(self.__config)\n",
    "    self.filter_param = filter_param\n",
    "\n",
    "  def document_exists(self) -> bool:\n",
    "    try:\n",
    "        if not self.vector_search_manager.index_exists():\n",
    "            return False\n",
    "        else:\n",
    "            query_text=\"*\"            \n",
    "            query_vector = self.__embedding_generator.generate_embeddings(query_text)\n",
    "            index = self.vector_search_manager.get_index()\n",
    "            res = index.similarity_search(query_vector=query_vector,columns=[\"page_content\",\"content_vector\"],filters={\"source\": self.__file_path})\n",
    "            result = res.get(\"result\")\n",
    "            return True if result and result.get(\"data_array\") else False\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Exception: {str(e)}\")\n",
    "\n",
    "  def process_documents(self):\n",
    "            \"\"\"\n",
    "            Uploads the documents to the vector database.\n",
    "            \"\"\"\n",
    "            image_descriptions = []\n",
    "            file_result = {\"file_name\":self.__file_name}\n",
    "            if self.document_exists():\n",
    "                file_result[\"status\"] = \"skipped\"\n",
    "                file_result[\"error\"] = \"Document already present\"\n",
    "                return file_result\n",
    "            _, file_extension = os.path.splitext(self.__file_name)\n",
    "            file_extension = file_extension[1:]\n",
    "            loader_config =  self.__config.get_loader_for_extension(file_extension)\n",
    "        # try:\n",
    "            if loader_config:\n",
    "                loader = globals()[loader_config.loader_class](file_path=self.__file_path,**loader_config.loader_kwargs)\n",
    "                documents = loader.load_and_split()\n",
    "                splitter = globals()[loader_config.splitter_class](**loader_config.splitter_kwargs)\n",
    "                documents = splitter.split_documents(documents)\n",
    "                if file_extension == \"pdf\":\n",
    "                    #### Image description from PDF\n",
    "                    try:\n",
    "                        image_extract = ImageExtract(self.__file_path)\n",
    "                        pdf_images = image_extract.load_file()\n",
    "                        image_collection = pdf_images[0].metadata[\"image_collection\"]\n",
    "                        img_urls = list(image_collection.keys())\n",
    "                        img_encodings = list(image_collection.values())\n",
    "                        for img_url, img_encoding in zip(img_urls, img_encodings):\n",
    "                            gptv4_service = AzureGpt4VService()\n",
    "                            image_descriptions = gptv4_service.image_description(\n",
    "                                images=[img_url],\n",
    "                                prompt=\"What is this image about?\",\n",
    "                                detail_mode=\"auto\",\n",
    "                                image_urls=[img_encoding],\n",
    "                                llm_endpoint=self.__config.get_vision_endpoint()                    \n",
    "                            )\n",
    "                            documents.extend(image_descriptions)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting image descriptions from PDF: {e}\")                       \n",
    "                    ### Table Extraction from PDF\n",
    "                    try:\n",
    "                        table_extraction = TableExtract(self.__file_path)\n",
    "                        tables = table_extraction.load_file()\n",
    "                        documents.extend(tables)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting tables from PDF: {e}\")\n",
    "\n",
    "                ## Data to store in vectorDB database\n",
    "                processed_documents = []\n",
    "                for page_num, doc in enumerate(documents, 1):                   \n",
    "                    embeddings = self.__embedding_generator.generate_embeddings(doc.page_content)\n",
    "                    if not doc.metadata:\n",
    "                        doc.metadata[\"page_number\"] = str(page_num)\n",
    "                        doc.metadata[\"source\"] = str(self.__file_path)                    \n",
    "                    for col in self.filter_param:\n",
    "                        doc.metadata[col] = doc.metadata.get(col, self.filter_param.get(col, \"\"))\n",
    "                    processed_documents.append({\"id\": f'{uuid.uuid4()}', \"page_content\": doc.page_content, \"source\": str(self.__file_path), \"metadata\": json.dumps(doc.metadata), \"content_vector\": embeddings, **{col: doc.metadata[col] for col in self.filter_param}})\n",
    "                print(f\"Uploading {len(processed_documents)} documents to vector database\")\n",
    "                self.vector_search_manager.add_documents(processed_documents)\n",
    "                file_result[\"status\"] = \"success\"\n",
    "                file_result[\"documents\"] = processed_documents            \n",
    "            else:\n",
    "                file_result[\"status\"] = \"failure\"\n",
    "                file_result[\"error\"] = f\"Unsupported file format:{self.__file_name}\"\n",
    "        # except Exception as e:\n",
    "        #     file_result[\"status\"] = \"failure\"\n",
    "        #     file_result[\"error\"] = str(e)\n",
    "            return file_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a620fc76-678d-46dc-bc54-9f2628cf42db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### DATA INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "config_path = \"/dbfs/mnt/regression_testing/hackathon_config/doc_config_01.json\"\n",
    "org_folder_path = '/dbfs/mnt/regression_testing1/hackathon_files/pending/'\n",
    "\n",
    "total_files = 0\n",
    "success_files = 0\n",
    "failure_files = 0\n",
    "skipped_files = 0\n",
    "process_start_time = dt.now()    \n",
    "for files in os.listdir(org_folder_path):\n",
    "    start_time = dt.now()\n",
    "    file_path = os.path.join(org_folder_path, files)\n",
    "    total_files += 1\n",
    "    print(f\"Processing {files}\")\n",
    "    upload_list = Upload(file_path=file_path, file_name=files, config_path=config_path, filter_param={})\n",
    "    result = upload_list.process_documents()\n",
    "    if result[\"status\"] == \"success\":\n",
    "        success_files += 1\n",
    "        print(f\"Success: {files}\")\n",
    "    elif result[\"status\"] == \"skipped\":\n",
    "        skipped_files += 1\n",
    "        print(f\"Skipped: {files}\")\n",
    "    else:\n",
    "        failure_files += 1\n",
    "        print(result[\"error\"])\n",
    "end_time = dt.now()\n",
    "total_time =  (end_time - start_time).total_seconds()\n",
    "print(f\"COMPANY Time Taken {total_time} seconds\")\n",
    "print(f\"Total files: {total_files}, Success files: {success_files}, Skipped files: {skipped_files}, Failure files: {failure_files}\")\n",
    "total_time = (dt.now() - process_start_time).total_seconds()\n",
    "print(f\"TOTAL PROCESSING TIME: {total_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Document_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
